{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ieremie\\Desktop\\TransformerGO\n",
      "C:\\Users\\Ieremie\\Desktop\\TransformerGO\\protein-ppi-encoding-module\n",
      "importing Jupyter notebook from protein2vec.ipynb\n",
      "C:\\Users\\Ieremie\\Desktop\\TransformerGO\n",
      "C:\\Users\\Ieremie\\Desktop\\TransformerGO\\datasets\n",
      "importing Jupyter notebook from dataset_manip.ipynb\n",
      "C:\\Users\\Ieremie\\Desktop\\TransformerGO\n",
      "C:\\Users\\Ieremie\\Desktop\\TransformerGO\\training-testing\n",
      "importing Jupyter notebook from training_helper.ipynb\n",
      "C:\\Users\\Ieremie\\Desktop\\TransformerGO\n"
     ]
    }
   ],
   "source": [
    "import import_ipynb\n",
    "\n",
    "#import model\n",
    "%cd ..\n",
    "%cd \"protein-ppi-encoding-module\"\n",
    "from protein2vec import *\n",
    "\n",
    "#import dataset functions\n",
    "%cd ..\n",
    "%cd \"datasets\"\n",
    "from dataset_manip import *\n",
    "%cd ..\n",
    "\n",
    "%cd \"training-testing\"\n",
    "from training_helper import * \n",
    "%cd ..\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data as data\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch.optim as optim\n",
    "import time\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import random\n",
    "from torch.optim import lr_scheduler\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import pickle\n",
    "from random import shuffle\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=np.VisibleDeprecationWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_path = \"datasets/jains-TCSS-datasets/yeast_data/iea+/negatives.sgd.iea.f\"\n",
    "poz_path = \"datasets/jains-TCSS-datasets/yeast_data/iea+/positives.sgd.iea.f\"\n",
    "\n",
    "go_embed_pth = \"term-encoding-module/emb/go-terms-128.emd\"\n",
    "go_id_dict_pth = \"term-encoding-module/go_id_dict\"\n",
    "protein_go_anno_pth = \"datasets/jains-TCSS-datasets/yeast_data/gene_association.sgd\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rejected interactions where at least one protein has no annotation:  0\n",
      "Rejected interactions where go_filter=ALL and intr_set_size_filter=[0, 500]:  0\n",
      "Number of interactions: 3858\n",
      "Rejected interactions where at least one protein has no annotation:  0\n",
      "Rejected interactions where go_filter=ALL and intr_set_size_filter=[0, 500]:  0\n",
      "Number of interactions: 3858\n"
     ]
    }
   ],
   "source": [
    "train_set, valid_set, test_set, full_dataset = get_dataset_split_stringDB(poz_path, neg_path, protein_go_anno_pth, go_id_dict_pth, go_embed_pth, shuffle, ratio = [0.8, 0.2, 0],  stringDB = False)\n",
    "\n",
    "MAX_LEN_SEQ = get_max_len_seq(full_dataset)\n",
    "def helper_collate(batch):\n",
    "    return batch_padding_collate_fn(batch, MAX_LEN_SEQ,  emb_dim = 128, global_padd = False)\n",
    "\n",
    "params = {'batch_size': 16,'collate_fn': helper_collate}\n",
    "train_grt = data.DataLoader(train_set, **params, shuffle = True)\n",
    "val_grt = data.DataLoader(valid_set, **params, shuffle = True)\n",
    "test_grt = data.DataLoader(test_set, **params, shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, criterion):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    model.train()\n",
    "    \n",
    "    pred = []\n",
    "    lab = []\n",
    "    for batch in iterator:\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        #batch tensor of shape N * 2(protein pair) * L(longest seq) * Emb dim\n",
    "        padded_pairs = batch[0].to(device)\n",
    "        labels = batch[1].to(device)\n",
    "        prots_A_len = batch[2]\n",
    "        prots_B_len = batch[3]\n",
    "        predictions = model(padded_pairs[:,0], padded_pairs[:,1], prots_A_len, prots_B_len).squeeze(1)\n",
    "        loss = criterion(predictions, labels)\n",
    "        acc = binary_accuracy(predictions, labels)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "        \n",
    "        pred = pred + list(predictions.cpu().data.numpy())\n",
    "        lab = lab + list(labels.cpu().data.numpy())\n",
    " \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator), roc_auc_score(lab,pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, iterator, criterion):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    pred = []\n",
    "    lab = []\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch in iterator:\n",
    "            \n",
    "            padded_pairs = batch[0].to(device)\n",
    "            labels = batch[1].to(device)\n",
    "            prots_A_len = batch[2]\n",
    "            prots_B_len = batch[3]\n",
    "            \n",
    "            predictions = model(padded_pairs[:,0], padded_pairs[:,1], prots_A_len, prots_B_len).squeeze(1)\n",
    "            loss = criterion(predictions, labels)\n",
    "            acc = binary_accuracy(predictions, labels)\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "            \n",
    "            pred = pred + list(predictions.cpu().data.numpy())\n",
    "            lab = lab + list(labels.cpu().data.numpy())\n",
    "\n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator), roc_auc_score(lab,pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_EPOCHS = 50\n",
    "DROPOUT = 0.2\n",
    "LR = 0.0005\n",
    "INPUT_DIM = 128  #node2vec embbedings\n",
    "HIDDEN_DIM = 64\n",
    "F_1 = 64\n",
    "F_2 = 16\n",
    "F_3 = 8\n",
    "F_4 = 1\n",
    "\n",
    "model = PROTEIN2VEC_SHARED(INPUT_DIM, HIDDEN_DIM, F_1, F_2, F_3, F_4, DROPOUT)\n",
    "model = model.to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=LR)\n",
    "criterion = nn.BCEWithLogitsLoss().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 50 | Epoch Time: 0m 14s \tTrain Loss: 0.227 | Train Acc: 91.16% \t Val. Loss: 0.373 |  Val. Acc: 86.24% \t Roc Train: 0.968 \t Roc Valid: 0.931 ,   0.0005 --LR\r"
     ]
    }
   ],
   "source": [
    "writer = SummaryWriter(flush_secs=14)\n",
    "N_EPOCHS = 50\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "\n",
    "    start_time = time.time()\n",
    "    train_loss, train_acc, roc_train = train(model, train_grt, optimizer, criterion)\n",
    "    valid_loss, valid_acc, roc_val = evaluate(model, val_grt, criterion)   \n",
    "    end_time = time.time()\n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    \n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(),  'model.pt')\n",
    "\n",
    "    print_status(epoch, epoch_mins, epoch_secs, train_loss,\\\n",
    "                 train_acc, valid_loss, valid_acc, roc_train, roc_val, optimizer)\n",
    "    write_scalars_tensorboard(writer, train_loss, valid_loss, train_acc, valid_acc, epoch)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load('model.pt'))\n",
    "test_loss, test_acc = evaluate(model, test_grt, criterion)\n",
    "print(f'Test Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 03 | Epoch Time: 0m 26s \tTrain Loss: 0.499 | Train Acc: 78.48% \t Val. Loss: 0.468 |  Val. Acc: 79.48% \t Roc Train: 0.837 \t Roc Valid: 0.877 ,   0.0005 --LR\r"
     ]
    }
   ],
   "source": [
    "C_FOLD = 5\n",
    "N_EPOCHS = 50\n",
    "DROPOUT = 0.2\n",
    "LR = 0.0005\n",
    "INPUT_DIM = 128  #node2vec embbedings\n",
    "HIDDEN_DIM = 64\n",
    "F_1 = 64\n",
    "F_2 = 16\n",
    "F_3 = 8\n",
    "F_4 = 1\n",
    "\n",
    "sz = len(full_dataset)\n",
    "fold_size = int(sz/C_FOLD)\n",
    "l = 0\n",
    "r = fold_size\n",
    "indexes = np.arange(sz)\n",
    "\n",
    "val_accs = []\n",
    "val_rocs = []\n",
    "wrong_eval = []\n",
    "wrong_eval_labels = []\n",
    "for i in range(0, C_FOLD):\n",
    "    print(\"Fold nr: \", i, end='\\r')\n",
    "    \n",
    "    model = PROTEIN2VEC_SHARED(INPUT_DIM, HIDDEN_DIM, F_1, F_2, F_3, F_4, DROPOUT)\n",
    "    model = model.to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=LR)\n",
    "    criterion = nn.BCEWithLogitsLoss().to(device)\n",
    "    writer = SummaryWriter(flush_secs=14)\n",
    "    \n",
    "    val_subset = data.Subset(full_dataset, indexes[l:r])\n",
    "    c_val_grt = data.DataLoader(val_subset, **params, shuffle = False)\n",
    "    \n",
    "    train_subset = data.Subset(full_dataset, np.concatenate([indexes[0:l], indexes[r:sz]]))\n",
    "    c_train_grt = data.DataLoader(train_subset, **params, shuffle = True)\n",
    "    \n",
    "    l += fold_size\n",
    "    r += fold_size\n",
    "\n",
    "    best_valid_roc = float('-inf')\n",
    "    best_valid_acc = float('-inf')\n",
    "    temp_w_eval = []\n",
    "    for epoch in range(N_EPOCHS):\n",
    "\n",
    "        start_time = time.time()\n",
    "        train_loss, train_acc, roc_train = train(model, train_grt, optimizer, criterion)\n",
    "        valid_loss, valid_acc, roc_val = evaluate(model, val_grt, criterion)\n",
    "        end_time = time.time()\n",
    "        epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    \n",
    "        print_status(epoch, epoch_mins, epoch_secs, train_loss,\\\n",
    "                 train_acc, valid_loss, valid_acc, roc_train, roc_val, optimizer)\n",
    "        write_scalars_tensorboard(writer, train_loss, valid_loss, train_acc, valid_acc, epoch)\n",
    "        \n",
    "        best_valid_roc = max(best_valid_roc, roc_val)\n",
    "        best_valid_acc = max(best_valid_acc, valid_acc)\n",
    "    \n",
    "    val_rocs.append(best_valid_roc)\n",
    "    val_accs.append(best_valid_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ieremie\\Desktop\\ppi-phd\\experiments-results\n"
     ]
    }
   ],
   "source": [
    "#saving the 5 cross validation results\n",
    "with open(\"5cv_roc_\" + neg_path[-24:][:4] + neg_path[-5:] + '.pkl', \"wb\") as fp:\n",
    "    pickle.dump(val_rocs, fp)\n",
    "with open(\"5cv_acc_\" + neg_path[-24:][:4] + neg_path[-5:] + '.pkl', \"wb\") as fp:\n",
    "    pickle.dump(val_accs, fp)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ppi-phd]",
   "language": "python",
   "name": "conda-env-ppi-phd-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
