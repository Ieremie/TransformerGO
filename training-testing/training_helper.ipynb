{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "lonely-childhood",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data as data\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "postal-mattress",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_padding_collate_fn(batch, MAX_LEN_SEQ, global_padd = False, emb_dim = 128):\n",
    "    \n",
    "    \"\"\" Function that remodels each batch of data before \n",
    "    input to the LSTM model\n",
    "\n",
    "    Args:\n",
    "        batch (tuple):  #batch_features:  Shape N * ( [protALen, node2vecDim], [protBLen, node2vecDim] )\n",
    "                        #batch_ids:       Shape N * ( [1, protALen], [1, protBLen] )\n",
    "                        #batch_labels:    Shape N * [ 1 or 0] \n",
    "\n",
    "        global_padd(bool): whether we are padding to the longest GO-set in dataset or the longest in batch\n",
    "\n",
    "    Returns:\n",
    "    tensor: padded proteins of shape N * 2(protein pair) * L(longest seq) * Emb dim\n",
    "    tensor: batch labels of   shape N * [ 1 or 0] \n",
    "    tensor: batch lengths of each protein A: e.g [22,10, ...] length N\n",
    "    tensor: batch lengths of each protein B\n",
    "    tensor: batch_ids of      shape N * ( [1, protALen], [1, protBLen] )\n",
    "    \"\"\"\n",
    "   \n",
    "    batch_features, batch_labels, batch_ids  = zip(*batch)\n",
    "    batch_features = np.array(batch_features)\n",
    "    unpadded_seqs = []\n",
    "    batch_protA_lenghts = []\n",
    "    batch_protB_lenghts = []\n",
    "    \n",
    "    for i in range(0, batch_features.shape[0]):\n",
    "        #append proteins pair\n",
    "        unpadded_seqs.append( torch.from_numpy(batch_features[i][0]))\n",
    "        unpadded_seqs.append( torch.from_numpy(batch_features[i][1])) \n",
    "        batch_protA_lenghts.append(len(batch_features[i][0]))\n",
    "        batch_protB_lenghts.append(len(batch_features[i][1]))\n",
    "        \n",
    "    if global_padd:\n",
    "        #pad proteins embedings according to the largest in the entire dataset\n",
    "        unpadded_seqs.append(torch.zeros(MAX_LEN_SEQ, emb_dim))                    \n",
    "        padded_seq = pad_sequence(unpadded_seqs, batch_first = True)\n",
    "        padded_seq = padded_seq[:-1]  \n",
    "    else:\n",
    "        #pad proteins embedings according to the largest in all proteins in the batch\n",
    "        padded_seq = pad_sequence(unpadded_seqs, batch_first = True)\n",
    "    \n",
    "    #create new tensor of shape N * 2(protein pair) * L(longest seq) * Emb dim\n",
    "    s = padded_seq.shape\n",
    "    padded_pairs = torch.empty((batch_features.shape[0], 2, s[1], s[2]))\n",
    "    #redo the pairs by skiping 1 protein\n",
    "    padded_pairs[:,0] = padded_seq[0::2]\n",
    "    padded_pairs[:,1] = padded_seq[1::2]\n",
    "    \n",
    "    return padded_pairs, torch.FloatTensor(batch_labels), \\\n",
    "           torch.FloatTensor(batch_protA_lenghts), torch.FloatTensor(batch_protB_lenghts), \\\n",
    "           np.array(batch_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "formal-present",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transformerGO_collate_fn(batch, max_size_set, emb_size = 64, pytorch_pad = False):\n",
    "     \n",
    "    \"\"\" Function that remodels each batch of data before \n",
    "    input to the transformer model\n",
    "\n",
    "    Args:\n",
    "        batch (tuple):  #batch_features:  Shape N * ( [protALen, node2vecDim], [protBLen, node2vecDim] )\n",
    "                        #batch_ids:       Shape N * ( [1, protALen], [1, protBLen] )\n",
    "                        #batch_labels:    Shape N * [ 1 or 0] \n",
    "\n",
    "    Returns:\n",
    "    tensor: padded proteins of shape N * 2(protein pair) * L(longest seq) * Emb dim\n",
    "    tensor: batch labels of   shape N * [ 1 or 0] \n",
    "    tensor: padding of        shape N * 2 * L * L\n",
    "    tensor: batch_ids of      shape N * ( [1, protALen], [1, protBLen] )\n",
    "    \"\"\"\n",
    "   \n",
    "    batch_features, batch_labels, batch_ids  = zip(*batch)\n",
    "    #batch_features = np.array((batch_features), dtype=object)\n",
    "    unpadded_seqs = []\n",
    "    padd_mask_pytorch = torch.ones((len(batch_features), 2, max_size_set), dtype=torch.bool)\n",
    "    padd_mask = torch.empty((len(batch_features), 2, max_size_set, max_size_set))\n",
    "    \n",
    "    for i in range(0, len(batch_features)):\n",
    "        protA = batch_features[i][0]\n",
    "        protB = batch_features[i][1]\n",
    "        unpadded_seqs.append( torch.FloatTensor(protA) )\n",
    "        unpadded_seqs.append( torch.FloatTensor(protB) ) \n",
    "        \n",
    "        #mask those positions which are not padding\n",
    "        padd_mask_pytorch[i][0][0:len(protA)] = False\n",
    "        padd_mask_pytorch[i][1][0:len(protB)] = False\n",
    "        \n",
    "    #pad proteins embedings according to the largest in the entire dataset\n",
    "    unpadded_seqs.append(torch.zeros(max_size_set, emb_size)) \n",
    "    padded_seq = pad_sequence(unpadded_seqs, batch_first = True)[:-1]\n",
    "\n",
    "    #create new tensor of shape N * 2(protein pair) * L(longest seq) * Emb dim\n",
    "    s = padded_seq.shape\n",
    "    padded_pairs = torch.empty((len(batch_features), 2, s[1], s[2]))\n",
    "    #redo the pairs by jumping 1 protein\n",
    "    padded_pairs[:,0] = padded_seq[0::2]\n",
    "    padded_pairs[:,1] = padded_seq[1::2]\n",
    "    \n",
    "    for i in range(0, padded_pairs.shape[0]):\n",
    "        padd_mask[i][0] = get_padd_mask_transformer(padded_pairs[i][0])\n",
    "        padd_mask[i][1] = get_padd_mask_transformer(padded_pairs[i][1])\n",
    "    \n",
    "    if pytorch_pad:\n",
    "        return padded_pairs, torch.FloatTensor(batch_labels), padd_mask_pytorch, np.array(batch_ids)\n",
    "    \n",
    "    return padded_pairs, torch.FloatTensor(batch_labels), padd_mask, np.array(batch_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "hindu-manchester",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_padd_mask_transformer(prot):\n",
    "    \"\"\"Gets an embedded protein and returns its padding mask\n",
    "    Args:\n",
    "        proteinA (numpy): numpy of shape (seqLen, emb_dim) \n",
    "\n",
    "    Returns:\n",
    "    numpy: matrix of size (seqLen, seqLen)\n",
    "    \"\"\"\n",
    "    mask = (prot.numpy() != 0)\n",
    "    mask = np.matmul(mask, mask.T)\n",
    "    return torch.from_numpy(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "commercial-weather",
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_accuracy(preds, y):\n",
    "    \"\"\"\n",
    "    Returns accuracy per batch, i.e. if you get 8/10 right, this returns 0.8, NOT 8\n",
    "    \"\"\"\n",
    "    #round predictions to the closest integer\n",
    "    rounded_preds = torch.round(torch.sigmoid(preds))\n",
    "    correct = (rounded_preds == y).float() #convert into float for division \n",
    "    acc = correct.sum() / len(correct)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "difficult-belfast",
   "metadata": {},
   "outputs": [],
   "source": [
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fleet-jerusalem",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_status(epoch, epoch_mins, epoch_secs, train_loss, train_acc, valid_loss, valid_acc, roc_train, roc_val, optimizer):\n",
    "    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s'  ,\n",
    "    f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%' , \n",
    "    f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%' , \n",
    "    f'\\t Roc Train: {roc_train:.3f}' , f'\\t Roc Valid: {roc_val:.3f}' , \n",
    "    \",  \", optimizer.param_groups[0]['lr'], \"--LR\", end='\\r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "variable-trading",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_scalars_tensorboard(writer, train_loss, valid_loss, train_acc, valid_acc, epoch):\n",
    "    writer.add_scalars('Loss', {'train':train_loss, 'valid': valid_loss}, epoch)\n",
    "    writer.add_scalars('Acc', {'train':train_acc, 'valid': valid_acc}, epoch) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ppi-phd]",
   "language": "python",
   "name": "conda-env-ppi-phd-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
