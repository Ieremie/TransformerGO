{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "heated-portuguese",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from harvard_transformer.ipynb\n"
     ]
    }
   ],
   "source": [
    "import import_ipynb\n",
    "from harvard_transformer import *\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "conditional-chest",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerGO(nn.Module):\n",
    "    def __init__(self ,d_model, nhead, num_layers, dim_feedforward, dropout = 0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        #encoder\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=nhead, dropout = dropout, dim_feedforward = dim_feedforward)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        \n",
    "        #decoder\n",
    "        decoder_layer = nn.TransformerDecoderLayer(d_model=d_model, nhead=nhead, dropout = dropout, dim_feedforward = dim_feedforward)\n",
    "        self.transformer_decoder = nn.TransformerDecoder(decoder_layer, num_layers=num_layers)\n",
    "        \n",
    "        #last linear layer\n",
    "        self.linear = nn.Linear(d_model, 1)\n",
    "    \n",
    "    #batch  * max_seq_len * node2vec_dim\n",
    "    def forward(self, emb_proteinA, emb_proteinB, protA_mask, protB_mask):\n",
    "        \n",
    "        memory = self.transformer_encoder(emb_proteinA, src_key_padding_mask = protA_mask)\n",
    "        output = self.transformer_decoder(emb_proteinB, memory, memory_key_padding_mask = protA_mask, tgt_key_padding_mask = protB_mask)\n",
    "        #output: seqLen * batch * embDim\n",
    "        \n",
    "        #transform B * seqLen * node2vec_dim --> B * node2vec_dim (TransformerCPI paper)\n",
    "        output = output.permute(1,0,2) \n",
    "        output_c = torch.linalg.norm(output, dim = 2)\n",
    "        output_c = F.softmax(output_c, dim = 1).unsqueeze(1)\n",
    "        output = torch.bmm(output_c, output)\n",
    "        \n",
    "        return self.linear(output).squeeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "romantic-emission",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerGO_Scratch(nn.Module):\n",
    "    def __init__(self ,d_model, nhead, num_layers, dim_feedforward, dropout = 0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        c = copy.deepcopy\n",
    "        attn = MultiHeadedAttention(nhead, d_model, dropout)\n",
    "        ff = PositionwiseFeedForward(d_model, dim_feedforward, dropout)\n",
    "        \n",
    "        self.encoder = Encoder(EncoderLayer(d_model, c(attn), c(ff), dropout), num_layers)\n",
    "        self.decoder = Decoder(DecoderLayer(d_model, c(attn), c(attn), c(ff), dropout), num_layers)\n",
    "\n",
    "        self.linear = nn.Linear(d_model, 1)\n",
    "    \n",
    "    #batch  * max_seq_len * node2vec_dim\n",
    "    def forward(self, emb_proteinA, emb_proteinB, protA_mask, protB_mask):\n",
    "        \n",
    "        memory = self.encoder(emb_proteinA, protA_mask)\n",
    "        output = self.decoder(emb_proteinB, memory, protA_mask, protB_mask)\n",
    "        #output: batch * seqLen * embDim\n",
    "        \n",
    "        #transform B * seqLen * node2vec_dim --> B * node2vec_dim (TransformerCPI paper)\n",
    "        output_c = torch.linalg.norm(output, dim = 2)\n",
    "        output_c = F.softmax(output_c, dim = 1).unsqueeze(1)\n",
    "        output = torch.bmm(output_c, output)\n",
    "        \n",
    "        return self.linear(output).squeeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5182ae2c-beb3-43d8-a5eb-2d35d408594d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GO_Sum_NN(nn.Module):\n",
    "    def __init__(self, input_dim, fc1_dim, fc2_dim, fc3_dim, fc4_dim, dropout):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.cos = nn.CosineSimilarity(dim=1, eps=1e-6)\n",
    "        self.fc1 = nn.Linear(input_dim * 2, fc1_dim)\n",
    "        self.fc2 = nn.Linear(fc1_dim, fc2_dim)\n",
    "        self.fc3 = nn.Linear(fc2_dim, fc3_dim)\n",
    "        self.fc4 = nn.Linear(fc3_dim, fc4_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    # emb_proteinA = (batch, max_seq_len, node2vec_dim)\n",
    "    #masks not used (adding 'empty' GO terms), addded only for downstream compatibility\n",
    "    def forward(self, emb_proteinA, emb_proteinB, maskA, maskB):\n",
    "        \n",
    "        emb_proteinA = torch.sum(emb_proteinA, dim = 1)\n",
    "        emb_proteinB = torch.sum(emb_proteinB, dim = 1)\n",
    "        \n",
    "        relu = nn.ReLU()        \n",
    "        fc1_output = self.fc1( torch.cat((emb_proteinA, emb_proteinB), dim=1) )\n",
    "        fc2_output = self.fc2( self.dropout( relu(fc1_output) ))\n",
    "        fc3_output = self.fc3( self.dropout( relu(fc2_output) ))\n",
    "        fc4_output = self.fc4( self.dropout( relu(fc3_output) ))\n",
    "        \n",
    "        return fc4_output"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ppi-phd]",
   "language": "python",
   "name": "conda-env-ppi-phd-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
