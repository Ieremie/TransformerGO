{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "import math\n",
    "import pickle\n",
    "import numpy as np\n",
    "import random\n",
    "from random import shuffle\n",
    "import torch\n",
    "import torch.utils.data as data\n",
    "\n",
    "import requests\n",
    "import re\n",
    "import gzip\n",
    "import collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_GO_explanation(go_terms, obo_csv_path = \"..\\\\term-encoding-module\\\\go-basic.obo.csv\"):\n",
    "    go_terms_csv = pd.read_csv(obo_csv_path)\n",
    "    go_terms = [ \"['\" + go + \"']\" for go in go_terms ]\n",
    "    go_terms_csv = go_terms_csv.query('id == @go_terms')\n",
    "    \n",
    "    #do the query one by one to keep the order (using list query does not maintain it)\n",
    "    final_query = go_terms_csv.query('id == @go_terms[0]')\n",
    "    for i in range(1, len(go_terms)):\n",
    "        go = go_terms[i]\n",
    "        query = go_terms_csv.query('id == @go')\n",
    "        final_query = final_query.append(query, ignore_index = True)\n",
    "    return final_query\n",
    "\n",
    "def trim_GO_expl(expls):\n",
    "    return [expl.strip(\"'[\").strip(\"]'\") for expl in expls ]\n",
    "\n",
    "def process_to_capital(onthologies):\n",
    "    cap = {\"['biological_process']\":\"BP\", \"['cellular_component']\":\"CC\", \"['molecular_function']\": \"MF\"}\n",
    "    return [cap[ontho] for ontho in onthologies] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_STRING_id_dict(aliases_path):\n",
    "    \"\"\" Functions that reads a file containing protein aliases\n",
    "          and returns this mapping as a dict\n",
    "    Args:\n",
    "        aliases_path (str): path to aliases file of proteins eg. ProtA is also known as kv010 in other databases    \n",
    "    Returns:\n",
    "    dict: dictionary of interaction aliases : 'ProtA' -> 'kv010'\n",
    "    \"\"\" \n",
    "    aliases = pd.read_table(aliases_path, delimiter = \"\\t\", compression = 'gzip', skiprows=[0])\n",
    "    #alias and proteinID\n",
    "    return dict(zip(aliases.iloc[:, 1],aliases.iloc[:, 0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_experiment_type_from_biogrid_stringDB_dataset(ppi_pth = \"onto2vec-datasets-string/data/train/9606.no-mirror.protein.links.v11.0.txt\", \\\n",
    "               exper_type_path = \"experiment-type-biogrid/BIOGRID-ORGANISM-Homo_sapiens-4.4.199.tab3.txt.gz\",  aliases_path = \"onto2vec-datasets-string/data/9606.protein.aliases.v11.0.txt.gz\"):\n",
    "      \n",
    "    \"\"\" Functions that reads protein interactions and generates a mapping to the type of interaction\n",
    "         e.g. HighThroughput or LowThroughput\n",
    "    Args:\n",
    "        ppi_pth (str): path to the interaction file\n",
    "        exper_type_path (str): path to the file containing information about the type of the interaction\n",
    "        aliases_path (str): path to aliases of proteins eg. ProtA is also known as kv010 in other databases\n",
    "        \n",
    "    Saves:\n",
    "    dict: dictionary of interaction type: IntrId -> 'High Throughput'\n",
    "    \"\"\" \n",
    "    \n",
    "    #upper is used to ensure no case sensitive matching are missed\n",
    "    aliases = get_STRING_id_dict(aliases_path)\n",
    "    aliases = {str(k).upper(): v  for k,v in aliases.items()}\n",
    "    exper_type = pd.read_table(exper_type_path, compression = 'gzip')\n",
    "        \n",
    "    #retrieve protein names from biogrid and then try to find a StringDB synonim\n",
    "    protAs = [aliases.get(str(protA).upper(), \"NOT_FOUND\") for protA in exper_type['Official Symbol Interactor A']]\n",
    "    protBs = [aliases.get(str(protB).upper(), \"NOT_FOUND\") for protB in exper_type['Official Symbol Interactor B']]\n",
    "    \n",
    "    # StringID-protA + StringID-protB -> Throughput, meaning interaction id to type of experiment\n",
    "    exper_type_dict = dict(zip(  [a+b for (a,b) in zip(protAs, protBs)],  exper_type['Throughput'])) \n",
    "    \n",
    "    print('Not found aliases in stringDB: ', len(['1' for x in protAs + protBs if 'NOT_FOUND' in x]))\n",
    "   \n",
    "    intrs = get_ppi_list(ppi_pth)\n",
    "    intr_exper_type = {}\n",
    "    row = 0\n",
    "    for (protA,protB) in intrs:\n",
    "        \n",
    "        intr_id = str(protA) + str(protB)\n",
    "        intr_id_mirror = str(protB) + str(protA)\n",
    "        \n",
    "        if intr_id in exper_type_dict:\n",
    "            intr_exper_type[intr_id] = exper_type_dict[intr_id]\n",
    "        elif intr_id_mirror in exper_type_dict:\n",
    "            intr_exper_type[intr_id] = exper_type_dict[intr_id_mirror]\n",
    "        else:\n",
    "            intr_exper_type[intr_id] = \"NOT_FOUND\"\n",
    "            \n",
    "    print(collections.Counter(intr_exper_type.values())) \n",
    "    with open(ppi_pth.split(\"/\")[-1] + \".experiment_type_dict\", 'wb') as handle:\n",
    "        pickle.dump(intr_exper_type, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def protein_annot_string_to_dict(annot_filename, aliases_path, unique = True):\n",
    "    \n",
    "    \"\"\" Functions that reads protein annotations and creates a dict \n",
    "    Args:\n",
    "        annot_filename (str): the path to the annotation file\n",
    "        aliases_path (str): path to aliases of proteins eg. ProtA is also known as kv010 in other databases\n",
    "        unique (bool): the inclusion of duplicate GO terms in the annotation dictionary\n",
    "    Returns:\n",
    "    dict: dictionary of annoatations: ProtId -> [GO1, GO2 ...]\n",
    "    \"\"\" \n",
    "    \n",
    "    string_id_dict =  get_STRING_id_dict(aliases_path)\n",
    "    protein_go_anno = defaultdict(list)\n",
    "    \n",
    "    with gzip.open(annot_filename,'rt', encoding='utf8', errors=\"ignore\") as f:\n",
    "        for line in f:\n",
    "            if line.startswith('!'): # Skip header\n",
    "                continue\n",
    "            line = line.strip().split('\\t')\n",
    "            protein_id = line[1]\n",
    "            go_term = line[4]\n",
    "            if line[6] == 'IEA' or line[6] == 'ND': # Ignore predicted or no data annotations\n",
    "                continue\n",
    "            if protein_id not in string_id_dict: # Not in StringDB\n",
    "                continue\n",
    "                \n",
    "            #get the id that corresponds to stringDB\n",
    "            string_id = string_id_dict[protein_id]\n",
    "            \n",
    "            if not unique:\n",
    "                protein_go_anno[string_id].append(go_term)\n",
    "            elif(go_term not in protein_go_anno[string_id] ):\n",
    "                 protein_go_anno[string_id].append(go_term)\n",
    "    return protein_go_anno\n",
    "\n",
    "def protein_annot_to_dict(annot_filename, unique = True):\n",
    "    \n",
    "    \"\"\" Functions that reads protein annotations and creates a dict \n",
    "    Args:\n",
    "        annot_filename (str): the path to the annotation file\n",
    "        unique (bool): the inclusion of duplicate GO terms in the annotation dictionary\n",
    "    Returns:\n",
    "    dict: dictionary of annoatations: ProtId -> [GO1, GO2 ...]\n",
    "    \"\"\" \n",
    "    gene_anno = pd.read_csv(annot_filename, sep = \"\\t\", header=None)  \n",
    "    protein_go_anno = defaultdict(list)\n",
    "\n",
    "    protein_ids = gene_anno[1].values\n",
    "    go_terms = gene_anno[4].values\n",
    "    for i in range (0,len(protein_ids)):\n",
    "        if not  unique:\n",
    "            protein_go_anno[protein_ids[i]].append(go_terms[i]) \n",
    "        elif( go_terms[i] not in protein_go_anno[protein_ids[i]] ):\n",
    "            protein_go_anno[protein_ids[i]].append(go_terms[i])   \n",
    "           \n",
    "    return protein_go_anno"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def go_embeddings_to_dict(go_embed_pth):\n",
    "    \n",
    "    \"\"\"\n",
    "    read the embeddings generated by Node2vec\n",
    "    :return dict of GOid -> embeddings\n",
    "    \"\"\"\n",
    "    #load the embeddings generated by node2vec for each index GO, ignore first information line\n",
    "    embeddings_dict = {}\n",
    "    embeddings = open(go_embed_pth).read().splitlines()[1:]\n",
    "    embeddings = [ x.split(\" \") for x in embeddings ]\n",
    "\n",
    "    for i in range(0, len(embeddings)):\n",
    "        #set the GO id as the key\n",
    "        key = int(embeddings[i][0]) \n",
    "        #add all the dimension of the embedings as a list of floats\n",
    "        embeddings_dict[key] = [ float(x) for x in embeddings[i][1:]]\n",
    "        \n",
    "    return embeddings_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ppi_list(ppi_pth):\n",
    "    \"\"\"reads the PPI file and \n",
    "    adds the interactins to a list of tuples\n",
    "    \"\"\"\n",
    "    ppi = []\n",
    "    with open(ppi_pth, \"r\") as f:  \n",
    "        ppi += [ (x.split(\",\")[0], x.split(\",\")[1].strip('\\n')) if ',' in x else\\\n",
    "                 (x.split(\"\\t\")[0], x.split(\"\\t\")[1].strip('\\n'))  for x in f ] \n",
    "    return ppi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_max_len_seq(dataset):\n",
    "    \"\"\"Finds the protein with the most annotations and returns the size\"\"\"\n",
    "    batch_features, batch_labels, batch_ids  = zip(*dataset)\n",
    "    batch_features = np.array(batch_features)\n",
    "    \n",
    "    max_len = 0\n",
    "    for i in range(0, batch_features.shape[0]):\n",
    "        max_len = max(max_len, len(batch_features[i][0]), len(batch_features[i][1]))\n",
    "    return max_len    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"This part of the code allows the model to generate embeddings on the go when there is a new batch generated.\n",
    "This is way more memory efficient than emmbeding the entire dataset and then keep it in memory.\n",
    "\"\"\"\n",
    "class Dataset_stringDB(torch.utils.data.Dataset):\n",
    "    #Characterizes a dataset for PyTorch\n",
    "    def __init__(self, all_ppi, labels, protein_go_anno, go_id_dict_pth, go_embed_pth,  shuffle, aliases_path, stringDB):\n",
    "        self.all_ppi = all_ppi\n",
    "        self.labels = labels\n",
    "        \n",
    "        #load the mapping from 'GO name' to index ex: GO0001 to 1\n",
    "        with open(go_id_dict_pth, 'rb') as fp:\n",
    "            go_id_dict = pickle.load(fp)\n",
    "        go_emb_dict = go_embeddings_to_dict(go_embed_pth)\n",
    "        \n",
    "        self.protein_go_anno = protein_go_anno\n",
    "        self.go_id_dict = go_id_dict\n",
    "        self.go_emb_dict = go_emb_dict\n",
    "        self.stringDB = stringDB\n",
    "        self.shuffle = shuffle\n",
    "    def __len__(self):\n",
    "        return len(self.all_ppi)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "\n",
    "        label = self.labels[index]    \n",
    "        protA,protB = self.all_ppi[index]\n",
    "        features, idi =  get_embedded_proteins(self.protein_go_anno, self.go_id_dict, self.go_emb_dict, self.shuffle, protA, protB)\n",
    "        return np.array((features, label, idi), dtype=object)\n",
    "\n",
    "def filter_interactions(ppi, protein_go_anno, go_id_dict_pth, aliases_path, stringDB, go_name_space_dict_pth, go_filter, intr_set_size_filter, max_intr_size):\n",
    "    \n",
    "    \"\"\" Checks the annotations of the interacting proteins for certain filters \n",
    "    Args:\n",
    "        ppi (list): the protein interactions ex. ['ProtA ProtB', ...]\n",
    "        protein_go_anno_pth (str): path to the annotation file\n",
    "        go_id_dict_pth (str): path to the dictionary of 'GOname -> 1', 1 representing the index found in node2vec edgelist\n",
    "        go_embed_pth (str): path to the embedings generated by node2vec for GO terms\n",
    "        aliases_path (str): path to the protein aliases \n",
    "        stringDB (bool): there are two different functions that generate dictionaries for protein annotations depending on the dataset\n",
    "        go_filter (str): what type of GO temrs to keep: ALL, CC, BP or MF        \n",
    "        intr_set_size_filter (list): the range of the the GO set size: e.g. [0,10] meaning the range from 0 to 10\n",
    "        max_intr_size (int): the maximum number of interactions to add to the dataset \n",
    "        \n",
    "    Returns:\n",
    "    list: returns only the valid interactions\n",
    "    \"\"\" \n",
    "\n",
    "    #load the mapping from 'GO name' to index ex: GO0001 -> 1\n",
    "    with open(go_id_dict_pth, 'rb') as fp:\n",
    "        go_id_dict = pickle.load(fp)\n",
    "        \n",
    "    #load the mapping from 'GO name' to namespace: GO0001 -> 'biological_process'\n",
    "    with open(go_name_space_dict_pth, 'rb') as fp:\n",
    "        go_name_space_dict = pickle.load(fp)\n",
    "    \n",
    "    filtered_ppi = []\n",
    "    rejected_no_annot = 0\n",
    "    rejected_filter = 0\n",
    "    for (protA, protB) in ppi:\n",
    "        \n",
    "        for prot in [protA, protB]:\n",
    "            #fileter those GO terms that are not found in the GO file used by us\n",
    "            protein_go_anno[prot] = [go for go in protein_go_anno[prot] if go in go_id_dict]\n",
    "\n",
    "            #Filter GO terms according to Ontology terms ('cellular', 'biological', 'moLecular')\n",
    "            if go_filter != \"ALL\":\n",
    "                protein_go_anno[prot] = [go for go in protein_go_anno[prot] if go_filter in go_name_space_dict[go]]\n",
    "        \n",
    "        #check if both proteins have atleast 1 GO term associated\n",
    "        if len(protein_go_anno[protA]) >0 and len(protein_go_anno[protB]) >0:\n",
    "            \n",
    "            # check if intr size is in range defined by filter\n",
    "            intr_set_size = (len(protein_go_anno[protA]) + len(protein_go_anno[protB]))\n",
    "            if intr_set_size >= intr_set_size_filter[0] and intr_set_size <= intr_set_size_filter[1]:\n",
    "                \n",
    "                #break if we added enoguh interactions to the dataset\n",
    "                if max_intr_size == len(filtered_ppi):\n",
    "                    return filtered_ppi, protein_go_anno\n",
    "                filtered_ppi.append((protA, protB))\n",
    "            else:\n",
    "                rejected_filter += 1\n",
    "        else:\n",
    "            rejected_no_annot += 1\n",
    "         \n",
    "    print(\"Rejected interactions where at least one protein has no annotation: \", rejected_no_annot)\n",
    "    print(f\"Rejected interactions where go_filter={go_filter} and intr_set_size_filter={intr_set_size_filter}: \", rejected_filter)\n",
    "    print(\"Number of interactions:\", len(filtered_ppi))\n",
    "    return filtered_ppi, protein_go_anno    \n",
    "    \n",
    "def get_embedded_proteins(protein_go_anno, go_id_dict, go_emb_dict, shuffle, protA, protB):\n",
    "    \n",
    "    \n",
    "    \"\"\" Embedding only 2 proteins\n",
    "    Args:\n",
    "        protein_go_anno_pth (str): path to the annotation file\n",
    "        go_id_dict (dict): dictionary of 'GOname -> 1', 1 representing the index found in node2vec edgelist\n",
    "        go_embed_dict (dict): embedings dict generated by node2vec for GO terms\n",
    "        shuffle (function): shuffle function for the GO term list, default is no shuffle\n",
    "        protA (string): protA name\n",
    "        protB (string): protB name\n",
    "        \n",
    "    Returns:\n",
    "    tuple: (protein embeddings, protein label information)\n",
    "    \"\"\" \n",
    "    \n",
    "   \n",
    "    \"\"\" Embedding only 2 proteins\n",
    "    Args:\n",
    "        protein_go_anno_pth (str): path to the annotation file\n",
    "        go_id_dict (dict): dictionary of 'GOname -> 1', 1 representing the index found in node2vec edgelist\n",
    "        go_embed_dict (dict): embedings dict generated by node2vec for GO terms\n",
    "        protA (string): protA name\n",
    "        protB (string): protB name\n",
    "        \n",
    "    Returns:\n",
    "    tuple: (protein embeddings, protein label information)\n",
    "    \"\"\" \n",
    "    \n",
    "    protein_go_anno[protA] = [go for go in protein_go_anno[protA] if go in go_id_dict]\n",
    "    protein_go_anno[protB] = [go for go in protein_go_anno[protB] if go in go_id_dict]\n",
    "    \n",
    "    emb_protA = [ go_id_dict[go] for go in protein_go_anno[protA] ]\n",
    "    emb_protB = [ go_id_dict[go] for go in protein_go_anno[protB] ]\n",
    "        \n",
    "    #shuffle if requierd for experiments\n",
    "    if shuffle is not None:\n",
    "        \n",
    "        #zip embeddings and GO labels to KEEP THE SAME MAPPING!!!!\n",
    "        shuffledA = list(zip(emb_protA, protein_go_anno[protA]))\n",
    "        shuffledB = list(zip(emb_protB, protein_go_anno[protB]))\n",
    "        \n",
    "        shuffle(shuffledA)\n",
    "        shuffle(shuffledB)\n",
    "        \n",
    "        emb_protA, protein_go_anno[protA] = zip(*shuffledA)\n",
    "        emb_protB, protein_go_anno[protB] = zip(*shuffledB)\n",
    "    \n",
    "    emb_protA = [ go_emb_dict[go] for go in emb_protA]\n",
    "    emb_protB = [ go_emb_dict[go] for go in emb_protB]\n",
    "    \n",
    "    # Shape ( [protALen, node2vecDim], [protBLen, node2vecDim] )\n",
    "    # Shape ( [1, protALen], [1, protBLen] )\n",
    "    return (np.array(emb_protA), np.array(emb_protB)), ((protA, protein_go_anno[protA]), (protB, protein_go_anno[protB]))\n",
    "\n",
    "\n",
    "def get_dataset_split_stringDB(poz_path, neg_path, protein_go_anno_pth, go_id_dict_pth, go_embed_pth, shuffle, aliases_path = \"\", ratio = [0.8, 0.2, 0],\\\n",
    "                               stringDB = True, go_name_space_dict_pth = \"datasets/transformerGO-dataset/go-terms/go_namespace_dict\", go_filter = \"ALL\", intr_set_size_filter = [0,500], max_intr_size = None):\n",
    "    \n",
    "    \"\"\" Splitting up the interaction data into train/valid/test and generating embeddings \n",
    "    Args:\n",
    "        poz_path (str): path to the positive interactions\n",
    "        poz_path (str): path to the negative interactions\n",
    "        protein_go_anno_pth (str): path to the annotation file\n",
    "        go_id_dict_pth (str): path to the the GO id dict, e.g. GO1 -> 1\n",
    "        shuffle (funct): function for shuffling the GO terms\n",
    "        aliases_path (str): path to the aliases file \n",
    "        ratio (list): list of 3 floats specifing the split between train/valid/test\n",
    "        stringDB (bool): weather we are using stringDB datasets or the Jains datasets\n",
    "        go_name_space_dict_pth (str): path to the dictionary between name of GO terms and explanation e.g. GO1 -> 'MF molecular function'\n",
    "        go_filter (str): filter for the GO terms (what terms to keep), e.g.  ALL, CC, BP, MF\n",
    "        intr_set_size_filter (list): the range of the the GO set size: e.g. [0,10] meaning the range from 0 to 10\n",
    "        max_intr_size (int): the maximum number of interactions to add to the dataset \n",
    "        \n",
    "    Returns:\n",
    "    Dataset_stringDB: 4 datasets objects for the train, valid, test and full datasets\n",
    "    \"\"\" \n",
    "    ppi_poz = get_ppi_list(poz_path)\n",
    "    ppi_neg = get_ppi_list(neg_path)  \n",
    "    \n",
    "    if stringDB:\n",
    "        protein_go_anno = protein_annot_string_to_dict(protein_go_anno_pth, aliases_path)\n",
    "    else:\n",
    "        protein_go_anno = protein_annot_to_dict(protein_go_anno_pth, True)\n",
    "    \n",
    "    ppi_poz, updated_protein_go_anno = filter_interactions(ppi_poz, protein_go_anno, go_id_dict_pth, aliases_path, stringDB, go_name_space_dict_pth, go_filter, intr_set_size_filter, max_intr_size)\n",
    "    ppi_neg, updated_protein_go_anno = filter_interactions(ppi_neg, updated_protein_go_anno, go_id_dict_pth, aliases_path, stringDB, go_name_space_dict_pth, go_filter, intr_set_size_filter, max_intr_size)\n",
    "    \n",
    "    all_ppi = ppi_poz + ppi_neg\n",
    "    labels =  [1] * len(ppi_poz) + [0] * len(ppi_neg)\n",
    "    ##shuffle the data such that the poz and neg don't appear toghether\n",
    "    full_dataset = list(zip(all_ppi, labels))\n",
    "    random.shuffle(full_dataset)\n",
    "    all_ppi, labels = zip(*full_dataset)\n",
    "    full_dataset = Dataset_stringDB(all_ppi, labels, updated_protein_go_anno, go_id_dict_pth, go_embed_pth, shuffle, aliases_path, stringDB)\n",
    "\n",
    "    sz = len(full_dataset)\n",
    "    train, valid, test = data.random_split(full_dataset, [int(ratio[0]*sz), int(ratio[1]*sz) , sz - (int(ratio[0]*sz) + int(ratio[1]*sz)) ] )\n",
    "\n",
    "    return train, valid, test, full_dataset"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ppi-phd]",
   "language": "python",
   "name": "conda-env-ppi-phd-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
